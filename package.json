{
  "name": "vscodium-agentic",
  "displayName": "Agentic Coding Assistant",
  "description": "Local LLM-powered agentic coding with llama.cpp",
  "version": "0.1.2",
  "publisher": "local",
  "engines": {
    "vscode": "^1.80.0"
  },
  "categories": [
    "AI",
    "Chat",
    "Programming Languages"
  ],
  "activationEvents": [
    "*"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "chatParticipants": [
      {
        "id": "agentic.local",
        "name": "agent",
        "fullName": "Local Agent",
        "description": "Agentic coding assistant powered by local LLMs",
        "isSticky": true
      }
    ],
    "viewsContainers": {
      "activitybar": [
        {
          "id": "agentic-sidebar",
          "title": "Agentic",
          "icon": "$(hubot)"
        }
      ]
    },
    "views": {
      "agentic-sidebar": [
        {
          "id": "agentic.chatView",
          "type": "webview",
          "name": "Local Agent Chat",
          "icon": "$(comment-discussion)"
        }
      ]
    },
    "commands": [
      {
        "command": "agentic.startServer",
        "title": "Agentic: Start LLM Server",
        "icon": "$(play)"
      },
      {
        "command": "agentic.stopServer",
        "title": "Agentic: Stop LLM Server",
        "icon": "$(stop)"
      },
      {
        "command": "agentic.selectModel",
        "title": "Agentic: Select Model",
        "icon": "$(database)"
      },
      {
        "command": "agentic.showStatus",
        "title": "Agentic: Show Status"
      },
      {
        "command": "agentic.openSettings",
        "title": "Agentic: Open Settings Panel",
        "icon": "$(gear)"
      },
      {
        "command": "agentic.openChat",
        "title": "Agentic: Open Chat Panel",
        "icon": "$(comment-discussion)"
      },
      {
        "command": "agentic.clearHistory",
        "title": "Agentic: Clear Conversation History"
      }
    ],
    "keybindings": [
      {
        "command": "agentic.openSettings",
        "key": "ctrl+shift+;",
        "mac": "cmd+shift+;"
      },
      {
        "command": "agentic.openChat",
        "key": "ctrl+shift+a",
        "mac": "cmd+shift+a"
      }
    ],
    "configuration": {
      "title": "Agentic Local LLM",
      "properties": {
        "agentic.llamaCpp.endpoint": {
          "type": "string",
          "default": "http://localhost:8080",
          "description": "URL of the llama.cpp server"
        },
        "agentic.llamaCpp.binaryType": {
          "type": "string",
          "default": "vulkan",
          "enum": [
            "cpu",
            "vulkan",
            "cuda"
          ],
          "description": "Hardware optimization for downloaded binaries. Choosing Vulkan is recommended for ROG Ally."
        },
        "agentic.llamaCpp.modelPath": {
          "type": "string",
          "default": "",
          "description": "Path to GGUF model file (for auto-starting server)"
        },
        "agentic.llamaCpp.autoStart": {
          "type": "boolean",
          "default": true,
          "description": "Automatically start the LLM server when extension activates"
        },
        "agentic.model.contextLength": {
          "type": "number",
          "default": 4096,
          "description": "Context length for the language model. Lower values (4096) are faster on CPU."
        },
        "agentic.model.maxTokens": {
          "type": "number",
          "default": 4096,
          "description": "Maximum tokens to generate per response"
        },
        "agentic.model.threads": {
          "type": "number",
          "default": 0,
          "description": "Number of threads to use for generation (set to 0 for auto-detect logical core count)"
        },
        "agentic.model.chatTemplate": {
          "type": "string",
          "default": "chatml",
          "enum": [
            "chatml",
            "auto",
            "llama3",
            "mistral-v7",
            "hermes-2-pro",
            "phi4",
            "gemma",
            "command-r",
            "llama2"
          ],
          "description": "Chat template format. ChatML works for most models (Falcon-H1, OpenHermes, Yi, etc). Use 'auto' to use model's built-in template."
        },
        "agentic.agent.maxIterations": {
          "type": "number",
          "default": 15,
          "description": "Maximum tool call iterations per request"
        },
        "agentic.tools.requireConfirmation": {
          "type": "boolean",
          "default": true,
          "description": "Require user confirmation before executing commands"
        },
        "agentic.tools.allowedCommands": {
          "type": "array",
          "default": [
            "npm",
            "node",
            "git",
            "python",
            "pip",
            "cargo",
            "rustc",
            "ls",
            "dir",
            "cat",
            "type",
            "echo",
            "pwd",
            "mkdir",
            "ls",
            "ps",
            "tasklist"
          ],
          "description": "Commands that can run without confirmation"
        },
        "agentic.sampling.temperature": {
          "type": "number",
          "default": 0.7,
          "minimum": 0,
          "maximum": 2,
          "description": "Sampling temperature (0 = deterministic, higher = more creative)"
        },
        "agentic.sampling.topP": {
          "type": "number",
          "default": 0.9,
          "minimum": 0,
          "maximum": 1,
          "description": "Top-P (nucleus) sampling threshold"
        },
        "agentic.expertApiKey": {
          "type": "string",
          "default": "",
          "description": "API key for cloud model. FREE options: Groq (groq.com), Together AI, or HuggingFace Inference"
        },
        "agentic.expertApiEndpoint": {
          "type": "string",
          "default": "https://api.groq.com/openai/v1/chat/completions",
          "description": "API endpoint. Groq (free): api.groq.com/openai/v1/chat/completions"
        },
        "agentic.expertModelName": {
          "type": "string",
          "default": "llama-3.3-70b-versatile",
          "description": "Model name. Groq free models: llama-3.3-70b-versatile, mixtral-8x7b-32768"
        },
        "agentic.sampling.topK": {
          "type": "number",
          "default": 40,
          "minimum": 1,
          "maximum": 100,
          "description": "Top-K sampling (number of top tokens to consider)"
        },
        "agentic.sampling.repeatPenalty": {
          "type": "number",
          "default": 1.1,
          "minimum": 1,
          "maximum": 2,
          "description": "Repetition penalty (1 = no penalty)"
        },
        "agentic.performance.flashAttention": {
          "type": "boolean",
          "default": true,
          "description": "Enable Flash Attention (-fa) for faster inference/prefill"
        },
        "agentic.performance.batchSize": {
          "type": "number",
          "default": 512,
          "description": "Batch size for prefill (-b)"
        },
        "agentic.performance.ubatchSize": {
          "type": "number",
          "default": 128,
          "description": "Physical batch size for prefill (-ub)"
        },
        "agentic.performance.cacheQuant": {
          "type": "string",
          "default": "f16",
          "enum": [
            "f16",
            "q8_0",
            "q4_0"
          ],
          "description": "KV cache quantization type (-ctk, -ctv). q8_0 or q4_0 saves VRAM/bandwidth but may lose quality."
        },
        "agentic.performance.cacheReuse": {
          "type": "number",
          "default": 0,
          "description": "KV cache reuse similarity threshold (0.0-1.0). Higher = stricter match. Set to 0 to disable."
        },
        "agentic.performance.noWarmup": {
          "type": "boolean",
          "default": false,
          "description": "Skip the initial warmup run of the model (--no-warmup)."
        },
        "agentic.performance.mlock": {
          "type": "boolean",
          "default": false,
          "description": "Force the system to keep the model in RAM (--mlock)."
        },
        "agentic.performance.mmap": {
          "type": "boolean",
          "default": true,
          "description": "Enable memory-mapped files (--mmap)."
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "lint": "eslint src --ext ts",
    "test": "jest"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "@types/vscode": "^1.80.0",
    "@typescript-eslint/eslint-plugin": "^6.0.0",
    "@typescript-eslint/parser": "^6.0.0",
    "eslint": "^8.0.0",
    "jest": "^29.0.0",
    "typescript": "^5.0.0"
  },
  "dependencies": {
    "@types/puppeteer-core": "^5.4.0",
    "puppeteer-core": "^22.15.0"
  }
}
